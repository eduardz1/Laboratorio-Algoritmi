\documentclass[12pt, letterpaper]{report}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{titlesec}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage[hidelinks]{hyperref}

\definecolor{mygreen}{RGB}{154,255,77}
\definecolor{mygray}{gray}{0.5}
\definecolor{myblue}{RGB}{41,141,255}

\lstset{
    language=C++,
    basicstyle=\tiny,
    keywordstyle=\color{myblue},
    commentstyle=\color{mygray},
    numberstyle=\color{mygreen},
    numbers=left,
    numberstyle=\tiny\color{mygray},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=2
}

\renewcommand{\thesection}{\arabic{section}} % removes reference of \chapter to avoid "0."
\titleformat{\chapter}{\normalfont\huge}{\thechapter}{20pt}{\huge\it}

\title{
    \leavevmode{\includegraphics[width=0.8\textwidth]{resources/Universita-degli-studi-di-torino-logo.png}\newline\newline}\\
    Relazione Algoritmi e Strutture Dati
}
\author{Eduard Antonovic Occhipinti, Iman Solaih, Marco Molica}

\begin{document}
\maketitle
\tableofcontents

\chapter*{Esercizio 1}
\section{Quick Sort}
Il \verb|quick_sort()| √® un algoritmo che ordina una collezione partendo da un pivot,
questo pu√≤ essere scelto in vari modi, e in base a quale viene scelto il tempo
di sorting varia. Il \verb|quick_sort()| utilizza \verb|_part()| per scegliere il pivot prima 
di chiamare \verb|partition()| per dividere gli elementi del range selezionato 
in un sottoinsieme di elementi maggiori e uno di elementi minori del pivot
la cui posizione finale viene restituita dal metodo.
\newline
\newline
Premessa: nella seguente relazione analizzeremo solo i dati raccolti su records
favorendo il primo \verb|field| nell'ordinamento, i dati per i restanti due field
sono equivalenti ma con costanti minori.

\newpage
\subsection{Impatto della scelta del pivot nel quick sort}
La tabella sottostante riporta il tempo impiegato ad ordinare un array di 
20 milioni elementi di tipo \verb|struct Record|
\begin{figure}[H]
\centering
    \include{figures/qs_boxplot_pivot}
\end{figure}
\newpage

La scelta del pivot diventa importante quando l'array in input risulta gi√†
parzialmente o totalmente ordinato.
Il grafico sottostante riporta il tempo impiegato da \verb|quick_sort()| 
per scorrere un array gi√† ordinato. Come ci aspettiamo, l'algoritmo degenera ad
$O(n^2)$, sia \verb|LAST| che \verb|FIRST| generano un grafico esponenziale ma con
costanti diverse, fosse l'array ordinato in ordine inverso ci aspettiamo il comportamento
opposto tra questi due.
\begin{figure}[H]
\centering
    \include{figures/qs_plot_pivot}
\end{figure}
Concentrandoci in particolare sui pivot \verb|median of 3|, \verb|random| e \verb|middle|, 
possiamo notare che per questi il tempo cresce in maniera costante.  
\begin{figure}[H]
\centering
    \include{figures/qs_plot_zoommed_pivot}
\end{figure}
In particolare \verb|MIDDLE| √® chiaramente il pivot con perfomance migliori, 
il risultato √® quello aspettato considerando che in questo contesto qui, 
\verb|partition()| non deve praticamente effettuare \verb|SWAP|. 
Possiamo comunque notare che il pivot \verb|RANDOM| si comporta discretamente, 
con una variabilit√† maggiore rispetto agli altri. \verb|MEDIAN3|
finir√† per sceglire lo stesso pivot di \verb|MIDDLE| e quindi il tempo aggiuntivo
√® interamente introdotto dall'overhead causato dal confronto dell'elemento centrale
con il first e last dell'array.

\newpage
\subsection{Fallback a Insertion Sort}
Quando il \verb|quick_sort()| lavora su un range sufficientemente piccolo, √® pi√π
efficiente utilizzare il \verb|insert_sort()|. Il range di cutoff √® stato impostato a 8 elementi.

\subsection{Scelta del partition}
Nel nostro dataset ogni \verb|record| √® virtualmente univoco, la partition di Lomuto
si comporta quindi molto bene ed anzi, secondo i nostri test, anche meglio di quella
di Hoare, nonostante quest'ultima infatti effettua meno \verb|SWAP|, √® pi√π complessa a
livello di codice e causa alla CPU una probabilit√† pi√π alta di branch misprediction.

\begin{lstlisting}
    template <typename T>
    int partition_lomuto(T array[], int left, int right)
    {
        T pivot = array[right];
        int i = left - 1;
        for (int j = left; j < right; j++){
            if (array[j] <= pivot) {
                i++;
                swap(&array[i], &array[j]);
            }
        }
        swap(&array[i + 1], &array[right]);
        return i + 1;
    }
\end{lstlisting}

Nel caso per√≤ si lavorasse su un dataset con una quantit√† importante di elementi
duplicati, la partition di Hoare inizia subito ad avere perfomance molto migliori,
una buona alternativa √® anche una partition di Lomuto modificata in maniera tale
da restituire due indici, dividendo quindi il subarray in tre parti:
elementi minori, uguali e maggiori del pivot.

\begin{lstlisting}
    template <typename T>
    int partition_hoare(T array[], int left, int right)
    {
        T pivot = array[(left + right) / 2];
        int i = left - 1;
        int j = right + 1;
        while (1) {
            do {
                i++;
            } while (array[i] < pivot);
            do {
                j--;
            } while (array[j] > pivot);
            if (i >= j) {
                return j;
            }
            swap(&array[i], &array[j]);
        }
    }
\end{lstlisting}

\newpage
\section{Binary Insertion Sort}`
Essendo l'algoritmo di complessit√† $O(n^2)$, non ci aspettiamo che finisca in tempi
sensati l'ordinamento dei 20 milioni di records, facendo due calcoli sui nostri computer
dovrebbe metterci approssimativmaente 2 anni.
Nel seguente schema possiamo per√≤ notare come la ricerca binaria del punto di inserimento
migliori notevolmente la costante di tempo.
\begin{figure}[H]
\centering
    \include{figures/bi_plot_unsorted}
\end{figure}

\chapter*{Esercizio 2}

\section{Skip List}
Dagli esperimenti effettuati i risultati dell'insertion mostrano come 
all'aumentare dei livelli il tempo di inserimento decresce in maniera esponenziale.
\begin{figure}[H]
\centering
    \include{figures/sklist_insert}
\end{figure}

Dal grafico si nota come la distribuzione dei livelli raggiunti √® concentrata 
attorno a 20, inoltre dal livello 30 in poi i livelli non vengono quasi mai raggiunti, 
difatti la probabilit√† di raggiungere ogni livello √® $\frac{1}{2^n}$, il livello
32, il massimo raggiunto, aveva probabilit√† $2.32 \times 10^{-10}$.
% üêñ (-(00)-) ewe TwT UwU  OwO B) :3 ywy üêé ü¶ö üê¨ ü¶π
% (._. ) ü§∫  (^_^') (>__>) (^x^ ) :-p~ 
% ‚°Ü‚£ê‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚†Ö‚¢ó‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚†ï‚†ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï
% ‚¢ê‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚£ï‚¢ï‚¢ï‚†ï‚†Å‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚†Ö‚°Ñ‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï
% ‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚†Ö‚¢ó‚¢ï‚†ï‚£†‚†Ñ‚£ó‚¢ï‚¢ï‚†ï‚¢ï‚¢ï‚¢ï‚†ï‚¢†‚£ø‚†ê‚¢ï‚¢ï‚¢ï‚†ë‚¢ï‚¢ï‚†µ‚¢ï
% ‚¢ï‚¢ï‚¢ï‚¢ï‚†Å‚¢ú‚†ï‚¢Å‚£¥‚£ø‚°á‚¢ì‚¢ï‚¢µ‚¢ê‚¢ï‚¢ï‚†ï‚¢Å‚£æ‚¢ø‚£ß‚†ë‚¢ï‚¢ï‚†Ñ‚¢ë‚¢ï‚†Ö‚¢ï
% ‚¢ï‚¢ï‚†µ‚¢Å‚†î‚¢Å‚£§‚£§‚£∂‚£∂‚£∂‚°ê‚£ï‚¢Ω‚†ê‚¢ï‚†ï‚£°‚£æ‚£∂‚£∂‚£∂‚£§‚°Å‚¢ì‚¢ï‚†Ñ‚¢ë‚¢Ö‚¢ë
% ‚†ç‚£ß‚†Ñ‚£∂‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£î‚¢ï‚¢Ñ‚¢°‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚°ë‚¢ï‚¢§‚†±‚¢ê
% ‚¢†‚¢ï‚†Ö‚£æ‚£ø‚†ã‚¢ø‚£ø‚£ø‚£ø‚†â‚£ø‚£ø‚£∑‚£¶‚£∂‚£Ω‚£ø‚£ø‚†à‚£ø‚£ø‚£ø‚£ø‚†è‚¢π‚£∑‚£∑‚°Ö‚¢ê
% ‚£î‚¢ï‚¢•‚¢ª‚£ø‚°Ä‚†à‚†õ‚†õ‚†Å‚¢†‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ä‚†à‚†õ‚†õ‚†Å‚†Ñ‚£º‚£ø‚£ø‚°á‚¢î
% ‚¢ï‚¢ï‚¢Ω‚¢∏‚¢ü‚¢ü‚¢ñ‚¢ñ‚¢§‚£∂‚°ü‚¢ª‚£ø‚°ø‚†ª‚£ø‚£ø‚°ü‚¢Ä‚£ø‚£¶‚¢§‚¢§‚¢î‚¢û‚¢ø‚¢ø‚£ø‚†Å‚¢ï
% ‚¢ï‚¢ï‚†Ö‚£ê‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚£ø‚£ø‚°Ñ‚†õ‚¢Ä‚£¶‚†à‚†õ‚¢Å‚£º‚£ø‚¢ó‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚¢ï‚°è‚£ò‚¢ï
% ‚¢ï‚¢ï‚†Ö‚¢ì‚£ï‚£ï‚£ï‚£ï‚£µ‚£ø‚£ø‚£ø‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£ï‚¢ï‚¢ï‚¢ï‚¢ï‚°µ‚¢Ä‚¢ï‚¢ï
% ‚¢ë‚¢ï‚†É‚°à‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚¢É‚¢ï‚¢ï‚¢ï
% ‚£Ü‚¢ï‚†Ñ‚¢±‚£Ñ‚†õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚¢Å‚¢ï‚¢ï‚†ï‚¢Å
% ‚£ø‚£¶‚°Ä‚£ø‚£ø‚£∑‚£∂‚£¨‚£ç‚£õ‚£õ‚£õ‚°õ‚†ø‚†ø‚†ø‚†õ‚†õ‚¢õ‚£õ‚£â‚£≠‚£§‚£Ç‚¢ú‚†ï‚¢ë‚£°‚£¥‚£ø
\begin{figure}[H]
\centering
    \include{figures/sklist_zoommed_insert}
\end{figure}

Bla bla bla facendo un grafico delle medie dei tempi di inserimento notiamo che 18
√® il numero ottimale di livelli
\begin{figure}[H]
\centering
    \include{figures/sklist_mean_insert}
\end{figure}

Bla bla bla search time decresce in maniera imporante
\begin{figure}[H]
\centering
    \include{figures/sklist_search}
\end{figure}

Bla bla bla in particolare zoommando sui livelli pi√π di interesse ci rendiamo conto
che la distribuzione √® concentrata attorno a 19
\begin{figure}[H]
\centering
    \include{figures/sklist_zoommed_search}
\end{figure}

Fcendo un grafico delle medie dei tempi di inserimento notiamo che 17
√® il numero ottimale di livelli
\begin{figure}[H]
\centering
    \include{figures/sklist_mean_search}
\end{figure}
Sorprendentemente il numero ottimale di livelli non coincide esattamente con $ln(n)$

\chapter*{Esercizio 3}
\section{Minimum Heap}
Per grantire la complessit√† in $O(1)$ della restituzione del \verb|left|, \verb|right|
e \verb|parent| di un elemento, a partire dal valore dello stesso, abbiamo usato 
una strttura dati di supporto, una \verb|HashMap<>|, che memorizza i valori degli 
elementi e li associa ai relativi indici nell' \verb|ArrayList| che rappresenta il 
nostro heap.
\newline
\newline
Abbiamo deciso di craere anche un'interfaccia \verb|PriorityQueue<>| che √® la Abstract
Data Structure sulla quale di basa il \verb|MinHeap<>|

\chapter*{Esercizio 4}
\section{Graph}
Abbiamo deciso di considerare il grafo diretto come la struttura dati base di un 
generico grafo, i grafi indiretto possono infatti essere visti come grafi diretti 
nei quali ad ogni arco viene associato anche un arco oppposto. Abbiamo deciso quindi 
di craere una classe \verb|UndirectGraph<>| che estende \verb|DirectGraph<>|, con 
costruttori \verb|protected|, ed una classe \verb|Graph<>| che incapsula i due.
\newline
\newline
Vi sono diversi modi di rappresentare un grafo G(V, E) a livello software, i due metodi 
pi√π intuitivi sono quelli della lista di adiacenza e della matrice di adiacenza.
La nostra implementazione sfrutta invece una mappa di vertici associati a mappe di 
vertici associati al \verb|weight| dell'arco. 
\newline
Concettualmente questa \verb|Map<V, Map<V, E>>| pu√≤ essere vista come una lista di 
adiacenza ma offre in realt√† tutti i vantaggi di una matrice di adiacenza.
\newline
\newline
Per aiutare nell'inizializzazione di un grafo, abbiamo deciso anche di creare una 
classe \verb|GraphBuilder<>| che sfruta il design pattern \verb|Builder|.

\section{Dijkstra}
Abbiamo implementato l' algoritmo di Dijkstra nella classe \verb|GraphHelper<>|, 
la classe contiene tutta una serie di methodi statici che possono essere di aiuto 
nell'utilizzo di un grafo. \newline
Abbiamo deciso di implementare l'algoritmo di Dijkstra qausi completamente generica.
Il itpo degli archi del grafo √® limitatao a \verb|E extends Number| principalmente 
per via dell'impossibilit√† in \verb|Java| di effettuare l'override degli \verb|operator|.
\newline
La funzione chiede che in input gli venga fornito, oltre all'oggetto grafo, l'elemento 
source e l'elemento destination, anche un \verb|Comparator<? super E>| che 
permetta di effettuare la comaprison tra i \verb|weight| degli archi, un "max" che ci 
permette di capire qual'√® il valore massimo di E (Ad esempio per \verb|Integer| basta inserire 
\verb|Integer.MAX_VALUE|). Il valore minimo √® quello che assumer√† il \verb|source| 
non appena inserito nella priority queue, il valore massimo invece √® quello al 
quale inizializziamo i vertici del grafo. \newline
La priority queue utilizzata per tenere traccia delle distanze tra \verb|source| e
i vari vertici √® il \verb|MinHeap<>|, gli elementi della priority queue sono dei 
\verb|Node<vertex, distance from source>|. \newline
Per memorizzare i predecessori e le distanze abbiamo deciso di usarre delle 
\verb|HashMap<>|. \newline
Abbiamo inoltre istanziato una mappa di valori a oggetti nodo in maniera tale da 
poter cercare in $O(1)$ gli elementi nella priority queue. \newline
L'algoritmo restituisce un \verb|Pair<>| che √® una coppia di elementi nel quale il
primo rappresenta il percorso minimo tra \verb|source| e \verb|destination| e il 
secondo contiene la distanza tra i due.


\end{document}